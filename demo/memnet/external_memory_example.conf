#edit-mode: -*- python -*-
# Copyright (c) 2016 Baidu, Inc. All Rights Reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from paddle.trainer_config_helpers import *
from external_memory import *

######################### parameters ###############################
is_test = get_config_arg('is_test', bool, False)
dict_dim = get_config_arg('dict_size', int, 8) # size of the dictionary
label_dim = dict_dim # the prediction has the same range as the input
word_embedding_dim = get_config_arg('word_emb_dim', int, 6) # dimension of the embedding
mem_fea_size = get_config_arg('size of each memory slot', int, 6)
mem_slot_size = get_config_arg('number of memory slots', int, 5)
controller_signal_dim = get_config_arg('the dim of the controller signal', int, 4)


######################## data source ################################                                                                                                                                       
if not is_test:
    define_py_data_sources2(train_list='dummy.list',
                        test_list=None,
                        module='data_provider_mem',
                        obj='process_seq_train')
else:
    define_py_data_sources2(train_list=None,
                        test_list='dummy.list',
                        module='data_provider_mem',
                        obj='process_seq_test')


settings(
    batch_size=10,
    learning_method=AdamOptimizer(),
    learning_rate=1e-3)


######################## network configure ################################   
data = data_layer(name="input_sequence", size=dict_dim)
gt_label = data_layer(name="ground_truth", size=label_dim)



emb = embedding_layer(input=data, size=word_embedding_dim)
 
def step_mem(y):
    external_memory = ExternalMemory('external_memory', mem_slot_size, mem_fea_size, False)
    rnn_memory = memory(name="rnn_memory",
                        size=controller_signal_dim,
                        boot_bias= ParamAttr(initial_std=0.0,
                                             initial_mean=0.))
    rnn_mem_out = mixed_layer(input = [full_matrix_projection(y),
                                       full_matrix_projection(rnn_memory)],
                                 bias_attr = None,
                                 act = LinearActivation(),
                                 name='rnn_memory',
                                 size = controller_signal_dim)

    control_signal = mixed_layer(input = [full_matrix_projection(y),
                                          full_matrix_projection(rnn_mem_out)],
                                 bias_attr = None,
                                 act = LinearActivation(),
                                 name = 'control_signal',
                                 size = controller_signal_dim)
    read_key = mixed_layer(input = [full_matrix_projection(y),
                                    full_matrix_projection(control_signal)],
                           bias_attr = None,
                           act = LinearActivation(),
                           size = mem_fea_size)
    memory_read = external_memory.read(read_key)
    write_key = mixed_layer(input = [full_matrix_projection(y),
                                     full_matrix_projection(control_signal)],
                            bias_attr = None,
                            act = LinearActivation(),
                            size = mem_fea_size)
    memory_out = external_memory.write(write_key)
    return memory_read



out = recurrent_group(
    name="rnn",
    step=step_mem,
    input=[emb])

if not is_test:
    out = last_seq(input=out)

pred = mixed_layer(input = full_matrix_projection(out),
                          bias_attr = True,
                          act = SoftmaxActivation(),
                          size = label_dim)



if is_test:
    pred = last_seq(input=pred)
    pred_id = maxid_layer(pred, name="prediction")
    print_layer(input=[data])
    print_layer(input=[gt_label])
    print_layer(input=[pred_id])

    cost = cross_entropy(input=pred, label=gt_label, name='cost_cls')
    outputs(cost)
else:
    outputs(classification_cost(input=pred,
                            label=gt_label))
